{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HyperParameters: Revisit this cell and update/add hyperparameters as build progresses\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "CONTEXT_LENGTH = 256\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EMBEDDING_DIM = 256\n",
    "NUM_QUERIES = 512\n",
    "EVAL_ITERS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_ITERS = 5000\n",
    "EVAL_INTERVAL = 500\n",
    "NUM_HEADS = 6\n",
    "DROPOUT_PROB = 0.2\n",
    "NUM_LAYERS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.replace('\\\\n', '\\n')\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "stoi = {char : idx for idx, char in enumerate(chars)}\n",
    "\n",
    "def encode(snippet):\n",
    "    return [stoi[char] for char in snippet]\n",
    "\n",
    "def decode(idx_list):\n",
    "    return ''.join(chars[idx] for idx in idx_list)\n",
    "\n",
    "## print(encode(\"wow\"), decode(encode(\"wow\")))\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train = data[:n]\n",
    "test = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(split = 'train'):\n",
    "    data = train if split == 'train' else test\n",
    "    idx_list = torch.randint(high = len(data) - CONTEXT_LENGTH, size = (BATCH_SIZE,))\n",
    "    x = torch.stack([data[idx : idx + CONTEXT_LENGTH] for idx in idx_list])\n",
    "    y = torch.stack([data[idx+1 : idx + 1 + CONTEXT_LENGTH] for idx in idx_list])\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# x, y = getBatch('val')\n",
    "# print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            X, Y = getBatch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_Head(nn.Module):\n",
    "    def __init__(self, head_size): ## FOr my understanding: hea_size is the no of queries\n",
    "        super().__init__()\n",
    "        self.Query = nn.Linear(EMBEDDING_DIM, head_size, bias=False)\n",
    "        self.Key = nn.Linear(EMBEDDING_DIM, head_size, bias=False)\n",
    "        self.Value = nn.Linear(EMBEDDING_DIM, head_size, bias=False)\n",
    "        # self.mask = torch.tril(torch.ones(CONTEXT_LENGTH, CONTEXT_LENGTH))\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(CONTEXT_LENGTH, CONTEXT_LENGTH)))\n",
    "\n",
    "        self.dropout = nn.Dropout(DROPOUT_PROB)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## I Thought: Input Shape = BATCH_SIZE, CONTEXT_LENGTH, EMBEDDING_DIM, Output_Shape = BATCH_SIZE, CONTEXT_LENGTH, head_size\n",
    "        ## Which is wrong because we'll send input at each timestamp. 1-2 chars will come as well not nessasarily a input of context length \n",
    "        # input of size (batch, time-step, channels) ## Channels = EMBEDDING_DIM\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        q = self.Query(x)   ## B, T, hs\n",
    "        k = self.Key(x)     ## B, T, hs\n",
    "        v = self.Value(x)   ## B, T, hs\n",
    "\n",
    "        attention = q @ k.transpose(-2, -1) ## B, T, T\n",
    "        ## Scale\n",
    "        attention /= (k.shape[-1]**0.5)\n",
    "        ## Apply Mask\n",
    "        attention = attention.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        ## softmax\n",
    "        attention = F.softmax(attention, dim = -1)\n",
    "        ## Dropout\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        out = attention @ v ## (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out ## (B, T, hs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([decoder_Head(head_size=head_size) for head_num in range(num_heads)])\n",
    "        self.ffn = nn.Linear(num_heads * head_size, EMBEDDING_DIM)\n",
    "\n",
    "        self.dropout = nn.Dropout(DROPOUT_PROB)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, T, C)\n",
    "        # Where B is the batch size, T is the timesteps, and C is the number of channels (embedding size)\n",
    "\n",
    "        # Applies forward method to each head in parallel\n",
    "        # Each head's output shape: (B, T, HS)\n",
    "        # Where HS is the head size\n",
    "        out = [head(x) for head in self.heads]\n",
    "\n",
    "        # Merge the head outputs along the channels (HS) axis\n",
    "        # Out shape: (B, T, Num_Heads * HS) or (B, T, QUERY_SIZE)\n",
    "        out = torch.cat(out, dim = -1)\n",
    "\n",
    "        # Pass the concatenated output through a Linear layer\n",
    "        # Final output shape: (B, T, EMBEDDING_DIM)\n",
    "        out = self.ffn(out)\n",
    "\n",
    "        ## Dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    ## Linear Feed Forward Layer with nonlinearities\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(DROPOUT_PROB),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoderBlock(nn.Module):\n",
    "    def __init__(self, n_queries, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = n_queries//n_head\n",
    "        self.multi_head_attn = MultiHeadAttention(num_heads=n_head, head_size=head_size)\n",
    "        self.layer_norm1 = nn.LayerNorm(EMBEDDING_DIM)\n",
    "        self.layer_norm2 = nn.LayerNorm(EMBEDDING_DIM)\n",
    "        self.ffn = FeedFoward(EMBEDDING_DIM)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (B, T, C)\n",
    "        # Where B is the batch size, T is the timesteps, and C is the number of channels (embedding size)\n",
    "\n",
    "        x_copy = x\n",
    "\n",
    "        ## Layer Norm 1 (Preserves shape)\n",
    "        x = self.layer_norm1(x)\n",
    "        \n",
    "        ## MultiheadAttention: (B, T, EMBEDDING_DIM)\n",
    "        x = self.multi_head_attn(x)\n",
    "        \n",
    "        ## Layer_Norm 2 (Add and Norm)\n",
    "        x += x_copy ## Preserves Shape\n",
    "        x_copy = x\n",
    "        x = self.layer_norm2(x)\n",
    "        \n",
    "        ## FFN\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        out = x + x_copy\n",
    "\n",
    "        return out ## (B, T, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Got it from some github repo\n",
    "\n",
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # Step 1\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # Step 2\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # Step 3\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Step 4\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Step 5\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # Step 6 ## (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)  # Step 7\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(0), :]  # Step 8\n",
    "        return self.dropout(x)  # Step 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yetAnotherGPT(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.lang_embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
    "        # self.pos_embedding = nn.Embedding(CONTEXT_LENGTH, EMBEDDING_DIM)\n",
    "        self.pos_encoder = PositionEncoding(d_model=EMBEDDING_DIM, max_len=CONTEXT_LENGTH)\n",
    "        # self.decoder = decoder_Head(NUM_QUERIES)\n",
    "        # head_size = NUM_QUERIES//NUM_HEADS\n",
    "        # self.decoder = MultiHeadAttention(num_heads=NUM_HEADS, head_size=head_size)\n",
    "        # self.Decoder_Block = decoderBlock(NUM_QUERIES, NUM_HEADS)\n",
    "        self.decoder_layers = nn.Sequential(*[decoderBlock(NUM_QUERIES, NUM_HEADS) for layer in range(NUM_LAYERS)])\n",
    "        self.layer_norm = nn.LayerNorm(EMBEDDING_DIM)\n",
    "        self.ffn_out = nn.Linear(EMBEDDING_DIM, vocab_size)\n",
    "    \n",
    "    def forward(self, x, y = None):\n",
    "        ## Again wrong here: x.shape = BATCH_SIZE, CONTEXT_LENGTH\n",
    "        ## X = Batch_size, TimeStamp\n",
    "\n",
    "        B, T = x.shape\n",
    "        l_embd = self.lang_embedding(x) ## B, T, EMBEDDING_DIM\n",
    "        # p_embd = self.pos_embedding(x) ## B, T, EMBEDDING_DIM\n",
    "        p_embd = self.pos_encoder(l_embd.transpose(1, 2)).transpose(1, 2)\n",
    "        x = l_embd + p_embd\n",
    "        x = self.decoder_layers(x) ## B, T, EMBEDDING_DIM\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.ffn_out(x) ## B, T, vocab_size\n",
    "        \n",
    "        loss = None\n",
    "\n",
    "        if y != None:\n",
    "            B, T, C = logits.shape ## BATCH, CONTEXT_SIZE, VOCAB_SIZE\n",
    "            logits = logits.view(B*T, C)\n",
    "            y = y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, context, max_new_tokens = 500):\n",
    "        # context is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(context[:, -CONTEXT_LENGTH:])\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            context = torch.cat((context, idx_next), dim=1) # (B, T+1)\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\raj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "model = yetAnotherGPT(vocab_size)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_ITERS):\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m EVAL_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 5\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      8\u001b[0m         X, Y \u001b[38;5;241m=\u001b[39m getBatch(split)\n\u001b[0;32m      9\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n\u001b[1;32m---> 10\u001b[0m         losses[split][k] \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     out[split] \u001b[38;5;241m=\u001b[39m losses[split]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iter in range(MAX_ITERS):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = getBatch('train')\n",
    "\n",
    "    # model forward pass and backward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the model: 257089\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "def count_parameters(param_tuple):\n",
    "    param, tensor = param_tuple\n",
    "    return tensor.nelement()\n",
    "total_params = sum(map(count_parameters, params))\n",
    "print(f'Total number of parameters in the model: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NARORI:\n",
      "Myvexe bevore, l Eyou are anny tha t?\n",
      "EOMyoro'tely wiyoou t, mt tied she-ours hear wi bl ind, ve ave,\n",
      "wean ame\n",
      "Thaccore wnimud hespl ishals dss bengeand arschoonorf d ild;\n",
      "Gove th 's\n",
      "Whe loveat Gicosthire toof bes?\n",
      "\n",
      "lve\n",
      "TAREUMARENIULO:\n",
      "AROBTICINHA hatth ITK:\n",
      "GES:\n",
      "Fu o-be tooust nower ct weid mour teseze hemy, Insth ak, woromyese! trie sstortsuson we, f oikine the:\n",
      "Hene\n",
      "Ove nthe se'tharo nde gur l-y pe lst, put hiserace:\n",
      "Ame ar yociend o sius alolllillled, s om I we s s: hathaso g ftt.\n",
      "Fo\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
    "gen = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "# print(gen.replace('\\\\n', '\\n'))\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
